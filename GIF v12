from future import annotations

import math
import time
import argparse
from dataclasses import dataclass
from typing import Dict, List, Tuple, Any, Optional
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.fft
import matplotlib.pyplot as plt

======================================================================

#CONFIGURATION

======================================================================

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.set_default_dtype(torch.float32)

def set_seed(seed=42):
torch.manual_seed(seed)
np.random.seed(seed)
if torch.cuda.is_available():
torch.cuda.manual_seed_all(seed)

@dataclass
class GIFConfig:
T: int = 400
d_in: int = 32
d_model: int = 64
d_out: int = 8
n_lanes: int = 4
k_modes: int = 16
window_size: int = 64
# Energy Weights
alpha: float = 5.0
beta: float = 1.0
gamma: float = 1.0
delta: float = 1.0
# Solver
gn_max_iter: int = 5
gn_tol: float = 1e-4
gn_damping: float = 1e-3
gn_cg_iters: int = 20
# Automaton
tau_enter_kl: float = 2.0
tau_enter_gn: float = 1.0
tau_exit: float = 1.0
kappa_thresh: float = 200.0
dwell_min: int = 5
# Training
lr: float = 1e-3
max_grad_norm: float = 1.0
seed: int = 42
batch_size: int = 8
epochs: int = 20

======================================================================

1. BACKBONE: RELATIVISTIC LANES (Full Implementation)

======================================================================

class StatefulGRU(nn.Module):
"""Explicit state handling for step-by-step intervention."""
def init(self, d_model):
super().init()
self.cell = nn.GRUCell(d_model, d_model)
def forward(self, x, h):
return self.cell(x, h)

class RelativisticLane(nn.Module):
def init(self, d_model, gamma_init):
super().init()
self.d_model = d_model
self.log_gamma = nn.Parameter(torch.log(torch.tensor(gamma_init)))
self.cell = StatefulGRU(d_model)

@property  
def gamma(self):  
    return torch.exp(self.log_gamma)  

def forward(self, x, h):  
    h_new = self.cell(x, h)  
    # Relativistic Gating  
    gate = torch.sigmoid(1.0 / (self.gamma + 1e-6))  
    return (1 - gate) * h + gate * h_new

class MultiLaneSystem(nn.Module):
def init(self, d_model, n_lanes):
super().init()
self.n_lanes = n_lanes
self.d_model = d_model
self.lanes = nn.ModuleList([
RelativisticLane(d_model, gamma_init=2.0**i)
for i in range(n_lanes)
])
self.fusion = nn.Linear(d_model * n_lanes, d_model)

def init_states(self, B):  
    return [torch.zeros(B, self.d_model, device=DEVICE) for _ in range(self.n_lanes)]  

def forward(self, x, states):  
    new_states = []  
    outputs = []  
    for i, lane in enumerate(self.lanes):  
        h_next = lane(x, states[i])  
        new_states.append(h_next)  
        outputs.append(h_next)  
    cat = torch.cat(outputs, dim=-1)  
    fused = self.fusion(cat)  
    return fused, new_states

======================================================================

2. MEMORY: TOEPLITZ-FFT + LANCZOS

======================================================================

def _next_power_of_two(n: int) -> int:
p = 1
while p < n: p <<= 1
return p

class ToeplitzKLSpectralMemory(nn.Module):
"""
Full Toeplitz-FFT Memory from v5.1.
No SVD shortcuts. Real Lanczos iteration.
"""
def init(self, d_model, k_modes, window_size, kernel_tau=None):
super().init()
self.d_model = d_model
self.k_modes = k_modes
self.window_size = window_size
self.buffer = []
self.kernel_tau = kernel_tau
self.register_buffer("H_hat", torch.zeros(1, d_model))
self.register_buffer("H_mean", torch.zeros(1, d_model)) # Added for projection centering
self.last_error = 0.0

def push(self, h):  
    self.buffer.append(h.detach())  
    if len(self.buffer) > self.window_size: self.buffer.pop(0)  

def refresh(self):  
    if len(self.buffer) < self.k_modes: return 0.0  
      
    # 1. Prepare History Matrix  
    N_hist = len(self.buffer)  
    H_ts = torch.stack([h.mean(dim=0) for h in self.buffer], dim=0)  
    self.H_mean = H_ts.mean(dim=0, keepdim=True)  
    H_centered = H_ts - self.H_mean  

    # 2. Toeplitz Kernel Construction  
    if self.kernel_tau is None: self.kernel_tau = max(1.0, self.window_size / 4.0)  
    idx = torch.arange(N_hist, device=DEVICE, dtype=torch.float32)  
    c = torch.exp(-0.5 * (idx / self.kernel_tau) ** 2)  

    # 3. FFT Circulant Embedding  
    L = 2 * N_hist - 1  
    L_fft = _next_power_of_two(L)  
    k_full = torch.zeros(L_fft, device=DEVICE)  
    k_full[:N_hist] = c  
    if N_hist > 1: k_full[-(N_hist - 1):] = c[1:].flip(0)  
    kernel_fft = torch.fft.rfft(k_full)  

    # Matvec Operator  
    def toeplitz_matvec(v):  
        v_pad = torch.zeros(L_fft, device=DEVICE, dtype=v.dtype)  
        v_pad[:N_hist] = v  
        res = torch.fft.irfft(kernel_fft * torch.fft.rfft(v_pad), n=L_fft)  
        return res[:N_hist]  

    # 4. Lanczos Iteration (The heavy lifting)  
    k_steps = min(self.k_modes * 4, N_hist)  
    Q, alpha, beta = [], [], []  
    q0 = torch.randn(N_hist, device=DEVICE)  
    q0 /= (torch.norm(q0) + 1e-8)  
    Q.append(q0)  

    for j in range(k_steps):  
        w = toeplitz_matvec(Q[-1])  
        a = torch.dot(Q[-1], w)  
        w = w - a * Q[-1]  
        if j > 0: w = w - beta[-1] * Q[-2]  
        b = torch.norm(w)  
        alpha.append(a)  
        if b < 1e-6: break  
        beta.append(b); Q.append(w/(b+1e-8))  

    m = len(alpha)  
    if m < 1: return 0.0  

    # 5. Ritz Reconstruction  
    T_mat = torch.zeros(m, m, device=DEVICE)  
    for i in range(m):  
        T_mat[i,i] = alpha[i]  
        if i < m-1: T_mat[i, i+1] = T_mat[i+1, i] = beta[i]  

    try:  
        _, evecs = torch.linalg.eigh(T_mat)  
    except: return 0.0  

    U_k = evecs[:, -min(self.k_modes, m):]  
    Q_mat = torch.stack(Q[:m], dim=1)  
    Phi_time = Q_mat @ U_k  
    Phi_time = Phi_time / (torch.norm(Phi_time, dim=0, keepdim=True) + 1e-8)  

    H_hat = Phi_time @ (Phi_time.T @ H_centered)  
    self.H_hat = H_hat  
      
    num = torch.norm(H_centered - H_hat)  
    den = torch.norm(H_centered) + 1e-8  
    self.last_error = (num/den).item()  
    return self.last_error  

def project(self, h):  
    if self.H_hat.numel() <= 1: return h  
    # Project onto stable manifold  
    last_proj = self.H_hat[-1]   
    return last_proj.unsqueeze(0).expand(h.shape[0], -1) + self.H_mean

======================================================================

3. SOLVER: TRUE GAUSS-NEWTON (HVP + CG)

======================================================================

class TrueGaussNewtonSolver:
"""
Double-Autograd Hessian-Vector Product Solver.
Handles flattening/unflattening of multi-lane states.
"""
def init(self, cfg):
self.max_iter = cfg.gn_max_iter
self.tol = cfg.gn_tol
self.damping = cfg.gn_damping
self.cg_iters = cfg.gn_cg_iters
self.last_update_norm = 0.0

def _cg(self, Avp_fn, b):  
    x = torch.zeros_like(b)  
    r = b.clone(); p = r.clone()  
    rs_old = torch.sum(r*r)  
    for _ in range(self.cg_iters):  
        Ap = Avp_fn(p)  
        alpha = rs_old / (torch.sum(p*Ap) + 1e-8)  
        x = x + alpha * p  
        r = r - alpha * Ap  
        rs_new = torch.sum(r*r)  
        if torch.sqrt(rs_new) < self.tol: break  
        p = r + (rs_new/(rs_old+1e-8)) * p  
        rs_old = rs_new  
    return x  

def solve(self, model, current_states, x_input, y_target):  
    """  
    Optimizes 'current_states' so that model(x_input, current_states) -> y_target.  
    Uses HVP to avoid instantiating the Hessian matrix.  
    """  
    # 1. Flatten list of states into single vector for optimizer  
    h_flat = torch.cat(current_states, dim=-1).detach().clone().requires_grad_(True)  
    x_in = x_input.detach()  
    y_t = y_target.detach()  
      
    # Helper to run model from flat state  
    def forward_flat(h):  
        # Split flat tensor back into list of states for the lanes  
        states_split = torch.chunk(h, model.temporal.n_lanes, dim=-1)  
        # Run the temporal step  
        fused, _ = model.temporal(x_in, states_split)  
        return model.readout(fused)  

    # 2. GN Loop  
    for _ in range(self.max_iter):  
        pred = forward_flat(h_flat)  
        loss = F.mse_loss(pred, y_t)  
          
        # Gradient (First Derivative)  
        g = torch.autograd.grad(loss, h_flat, create_graph=True)[0]  
          
        # HVP Operator (Second Derivative * Vector)  
        def Avp(v):  
            dot = torch.sum(g * v)  
            # Double backward trick  
            Hv = torch.autograd.grad(dot, h_flat, retain_graph=True)[0]  
            return Hv + self.damping * v  
          
        # CG Solve for update direction  
        delta = self._cg(Avp, -g)  
          
        # Update  
        h_flat = h_flat + delta  
        self.last_update_norm = delta.norm().item()  
          
    # 3. Unpack and Return  
    final_states = list(torch.chunk(h_flat.detach(), model.temporal.n_lanes, dim=-1))  
    return final_states

======================================================================

4. AUTOMATON & ENERGY (Full Classes)

======================================================================

class EnergyFunctional:
def init(self, cfg):
self.cfg = cfg

def calc(self, e_pred, e_kl, kappa, update_norm):  
    return (self.cfg.alpha * e_pred +   
            self.cfg.beta * e_kl +   
            self.cfg.gamma * math.log(kappa + 1e-8) +   
            self.cfg.delta * update_norm)

class SRHISAutomaton:
def init(self, cfg):
self.cfg = cfg
self.mode = "FAST"
self.ema_E = 0.0
self.ema_var = 1.0
self.dwell = 0

def step(self, E_t, kappa):  
    delta = E_t - self.ema_E  
    self.ema_E = 0.9 * self.ema_E + 0.1 * E_t  
    self.ema_var = 0.9 * self.ema_var + 0.1 * delta**2  
    z = delta / (math.sqrt(self.ema_var) + 1e-8)  
      
    self.dwell += 1  
      
    # Full Hysteresis Logic  
    if self.mode == "FAST":  
        if kappa > self.cfg.kappa_thresh: self.mode = "KL"; self.dwell=0  
        elif z > self.cfg.tau_enter_gn: self.mode = "GN"; self.dwell=0  
        elif z > self.cfg.tau_enter_kl: self.mode = "KL"; self.dwell=0  
    elif self.mode == "KL":  
        if kappa > self.cfg.kappa_thresh: self.mode = "KL" # Stick  
        elif z > self.cfg.tau_enter_gn: self.mode = "GN"; self.dwell=0  
        elif z < self.cfg.tau_exit and self.dwell > self.cfg.dwell_min: self.mode = "FAST"  
    elif self.mode == "GN":  
        if kappa > self.cfg.kappa_thresh: self.mode = "KL"; self.dwell=0 # Abort to KL  
        elif z < self.cfg.tau_exit and self.dwell > self.cfg.dwell_min: self.mode = "FAST"  
          
    return self.mode, E_t, z

======================================================================

5. SYSTEM WRAPPER

======================================================================

class GIF_SRHIS_Omnibus(nn.Module):
def init(self, cfg: GIFConfig):
super().init()
self.cfg = cfg
self.in_proj = nn.Linear(cfg.d_in, cfg.d_model)
self.temporal = MultiLaneSystem(cfg.d_model, cfg.n_lanes)
self.readout = nn.Linear(cfg.d_model, cfg.d_out)

self.memory = ToeplitzKLSpectralMemory(cfg.d_model, cfg.k_modes, cfg.window_size)  
    self.solver = TrueGaussNewtonSolver(cfg)  
    self.automaton = SRHISAutomaton(cfg)  
    self.energy_fn = EnergyFunctional(cfg)  

def run_loop(self, X, Y):  
    T, B, _ = X.shape  
    states = self.temporal.init_states(B)  
    preds = []  
    log = {"mode": [], "energy": [], "z": [], "kappa": []}  
      
    optimizer = optim.AdamW(self.parameters(), lr=self.cfg.lr)  
      
    print(f"Running Omnibus Loop (T={T})...")  
      
    for t in range(T):  
        self.train()  
        optimizer.zero_grad()  
        x_t, y_true = X[t], Y[t]  
          
        # 1. PREDICT & COMMIT (Honest)  
        x_emb = self.in_proj(x_t)  
        fused, next_states_candidate = self.temporal(x_emb, states)  
        y_pred = self.readout(fused)  
        preds.append(y_pred.detach())   
          
        # 2. MEASURE  
        with torch.no_grad():  
            e_pred = F.mse_loss(y_pred, y_true).item()  
              
            # Refresh memory (Every step for max fidelity, or strided)  
            if t % 5 == 0: self.memory.push(fused)  
            e_kl = self.memory.refresh() if len(self.memory.buffer) > 10 else 0.0  
              
            # Kappa Estimation (Simplified SVD on current buffer)  
            kappa = 1.0  
            if len(self.memory.buffer) > 10:  
                h_sample = torch.stack(self.memory.buffer[-10:]).mean(dim=1)  
                try:  
                    S = torch.linalg.svdvals(h_sample)  
                    kappa = (S[0] / (S[-1]+1e-6)).item()  
                except: pass  
              
            update_norm = self.solver.last_update_norm  
            E_t = self.energy_fn.calc(e_pred, e_kl, kappa, update_norm)  
            mode, _, z = self.automaton.step(E_t, kappa)  
              
        # 3. CORRECT  
        states_for_next = next_states_candidate  
          
        if mode == "FAST":  
            loss = F.mse_loss(y_pred, y_true)  
            loss.backward(); optimizer.step()  
              
        elif mode == "KL":  
            # Project onto manifold  
            with torch.no_grad():  
                states_for_next = [self.memory.project(s) for s in next_states_candidate]  
            loss = F.mse_loss(y_pred, y_true)  
            loss.backward(); optimizer.step()  
              
        elif mode == "GN":  
            # Solver Intervention: Correct INPUT states for this step  
            corrected_states = self.solver.solve(self, states, x_emb, y_true)  
              
            # Recalculate NEXT states based on corrected input  
            with torch.no_grad():  
                _, improved_next = self.temporal(x_emb, corrected_states)  
            states_for_next = improved_next  
              
        # 4. STEP  
        states = [s.detach() for s in states_for_next]  
          
        log["mode"].append(mode)  
        log["energy"].append(E_t)  
        log["z"].append(z)  
        log["kappa"].append(kappa)  
          
    return torch.stack(preds), log

======================================================================

6. RUNNER

======================================================================

if name == "main":
cfg = GIFConfig()
set_seed(cfg.seed)

# Data Generation  
t = torch.arange(cfg.T, device=DEVICE).float().unsqueeze(-1).unsqueeze(-1)  
freqs = torch.logspace(-1.5, 0.0, cfg.d_in//2, device=DEVICE)  
sig = torch.cat([torch.sin(t*freqs), torch.cos(t*freqs)], -1).repeat(1, cfg.batch_size, 1)  
X = sig + 1.5 * torch.randn_like(sig) # Heavy noise  
# Surprise event  
X[200:220] *= 3.0   
W = torch.randn(cfg.d_in, cfg.d_out, device=DEVICE) * 0.5  
Y = torch.einsum("tbf,fo->tbo", sig, W)  
  
# Model  
model = GIF_SRHIS_Omnibus(cfg).to(DEVICE)  
  
# Run  
start = time.time()  
preds, log = model.run_loop(X, Y)  
end = time.time()  
  
mse = F.mse_loss(preds, Y).item()  
  
print("\n" + "="*60)  
print("SRHIS-v12: OMNIBUS PROOF")  
print("="*60)  
print(f"MSE: {mse:.5f}")  
print(f"Time: {end-start:.2f}s")  
print("-" * 20)  
print(f"FAST: {log['mode'].count('FAST')}")  
print(f"KL: {log['mode'].count('KL')}")  
print(f"GN: {log['mode'].count('GN')}")  
  
# Visualization  
plt.figure(figsize=(12, 10))  
  
plt.subplot(4,1,1)  
plt.plot(preds[:,0,0].cpu(), label="Pred")  
plt.plot(Y[:,0,0].cpu(), alpha=0.6, label="True")  
plt.title("Trajectory (Dim 0)")  
plt.legend()  
  
plt.subplot(4,1,2)  
plt.plot(log["energy"], color='red')  
plt.title("Unified Energy")  
  
plt.subplot(4,1,3)  
plt.plot(log["kappa"], color='orange')  
plt.yscale('log')  
plt.title("Condition Number (Kappa)")  
  
plt.subplot(4,1,4)  
m_map = {"FAST":0, "KL":1, "GN":2}  
plt.plot([m_map[m] for m in log["mode"]], color='purple')  
plt.yticks([0,1,2], ["FAST","KL","GN"])  
plt.title("Automaton Mode")  
  
plt.tight_layout()  
plt.savefig("srhis_v12_omnibus.png")  
print("Saved srhis_v12_omnibus.png")
